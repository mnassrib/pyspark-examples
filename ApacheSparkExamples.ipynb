{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foreword: Spark Overview\n",
    "Apache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.\n",
    "\n",
    "## Machine Learning Library (MLlib)\n",
    "MLlib is Spark’s machine learning (ML) library. Its goal is to make practical machine learning scalable and easy. At a high level, it provides tools such as:\n",
    "\n",
    "- ML Algorithms: common learning algorithms such as classification, regression, clustering, and collaborative filtering\n",
    "- Featurization: feature extraction, transformation, dimensionality reduction, and selection\n",
    "- Pipelines: tools for constructing, evaluating, and tuning ML Pipelines\n",
    "- Persistence: saving and load algorithms, models, and Pipelines\n",
    "- Utilities: linear algebra, statistics, data handling, etc.\n",
    "\n",
    "> ### Announcement: DataFrame-based API is primary API\n",
    "\n",
    "> **The MLlib RDD-based API is now in maintenance mode.**\n",
    "\n",
    "As of Spark 2.0, the RDD-based APIs in the ``spark.mllib`` package have entered maintenance mode. The primary Machine Learning API for Spark is now the DataFrame-based API in the ``spark.ml`` package.\n",
    "\n",
    "What are the implications?\n",
    "\n",
    "- MLlib will still support the RDD-based API in spark.mllib with bug fixes.\n",
    "- MLlib will not add new features to the RDD-based API.\n",
    "- In the Spark 2.x releases, MLlib will add features to the DataFrames-based API to reach feature parity with the RDD-based API.\n",
    "- After reaching feature parity (roughly estimated for Spark 2.3), the RDD-based API will be deprecated.\n",
    "- The RDD-based API is expected to be removed in Spark 3.0.\n",
    "\n",
    "> **Why is MLlib switching to the DataFrame-based API?**\n",
    "\n",
    "- DataFrames provide a more user-friendly API than RDDs. The many benefits of DataFrames include Spark Datasources, SQL/DataFrame queries, Tungsten and Catalyst optimizations, and uniform APIs across languages.\n",
    "- The DataFrame-based API for MLlib provides a uniform API across ML algorithms and across multiple languages.\n",
    "- DataFrames facilitate practical ML Pipelines, particularly feature transformations. See the Pipelines guide for details.\n",
    "\n",
    "What is “Spark ML”?\n",
    "\n",
    "- **“Spark ML” is not an official name but occasionally used to refer to the MLlib DataFrame-based API.** This is majorly due to the org.apache.spark.ml Scala package name used by the DataFrame-based API, and the “Spark ML Pipelines” term we used initially to emphasize the pipeline concept.\n",
    "\n",
    "Is MLlib deprecated?\n",
    "\n",
    "- No. MLlib includes both the RDD-based API and the DataFrame-based API. The RDD-based API is now in maintenance mode. But neither API is deprecated, nor MLlib as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark Examples\n",
    "These examples give a quick overview of the Spark API. Spark is built on the concept of distributed datasets, which contain arbitrary Java or Python objects. You create a dataset from external data, then apply parallel operations to it. The building block of the Spark API is its RDD API. In the RDD API, there are two types of operations: transformations, which define a new dataset based on previous ones, and actions, which kick off a job to execute on a cluster. On top of Spark’s RDD API, high level APIs are provided, e.g. DataFrame API and Machine Learning API. These high level APIs provide a concise way to conduct certain data operations. In this page, we will show examples using RDD API as well as examples using high level APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the version of python installed on your system, you can go through the sys module by typing in a python script the following lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.6 |Anaconda, Inc.| (default, Jun 28 2018, 11:27:44) [MSC v.1900 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD-based APIs Examples (``spark.mllib`` package)\n",
    "Spark revolves around the concept of a resilient distributed dataset (RDD), which is a fault-tolerant collection of elements that can be operated on in parallel. There are two ways to create RDDs: parallelizing an existing collection in your driver program, or referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat.\n",
    "\n",
    "\n",
    "#### RDD Operations\n",
    "RDDs support two types of operations: **transformations**, which create a new dataset from an existing one, and **actions**, which return a value to the driver program after running a computation on the dataset. For example, map is a transformation that passes each dataset element through a function and returns a new RDD representing the results. On the other hand, reduce is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program (although there is also a parallel reduceByKey that returns a distributed dataset).\n",
    "\n",
    "All transformations in Spark are lazy, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program. This design enables Spark to run more efficiently. For example, we can realize that a dataset created through map will be used in a reduce and return only the result of the reduce to the driver, rather than the larger mapped dataset.\n",
    "\n",
    "By default, each transformed RDD may be recomputed each time you run an action on it. However, you may also persist an RDD in memory using the persist (or cache) method, in which case Spark will keep the elements around on the cluster for much faster access the next time you query it. There is also support for persisting RDDs on disk, or replicated across multiple nodes.\n",
    "\n",
    "RDDs have two types of methods:\n",
    "\n",
    "    - The transformations that output another RDD.\n",
    "\n",
    "    - Actions that output something other than a RDD.\n",
    "\n",
    "#### Spark Transformations\n",
    "- map()\n",
    "- flatMap()\n",
    "- mapPartitions()\n",
    "- filter()\n",
    "- sample()\n",
    "- union()\n",
    "- intersection()\n",
    "- distinct()\n",
    "- join()\n",
    "- ...\n",
    "\n",
    "#### Spark Actions\n",
    "- reduce()             \n",
    "- collect()               \n",
    "- count()\n",
    "- first()    \n",
    "- takeSample(withReplacement, num, [seed]) \n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Word Count\n",
    "In this example, we use a few transformations to build a dataset of (String, Int) pairs called counts and then save it to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"ALICE'S\", 3), ('ADVENTURES', 1), ('', 1924), ('Lewis', 1), ('Carroll', 1), ('MILLENNIUM', 1), ('FULCRUM', 1), ('EDITION', 1), ('I.', 1), ('Down', 1)]\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "## write snappy compressed output\n",
    "\n",
    "sparkConf = SparkConf().setAll([\n",
    "    (\"spark.cores.max\", \"4\"),\n",
    "    (\"spark.executor.memory\", \"2G\"),\n",
    "    (\"spark.ui.port\", \"8080\")\n",
    "    ]).setMaster(\"local[*]\").setAppName(\"Word Count\")\n",
    " \n",
    "sc = SparkContext(conf=sparkConf)\n",
    "\n",
    "import os, shutil\n",
    "dir_name = \"pyspark-examples\"\n",
    "if os.path.isdir(dir_name):\n",
    "    shutil.rmtree(dir_name)\n",
    "       \n",
    "text_file = sc.textFile(\"data/wonderland.txt\")\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# To display the counting result, use the following action:\n",
    "#counts.collect()\n",
    "# To save the counting result in text files, use the following action:\n",
    "#counts.saveAsTextFile(\"wordcountresults.txt\")\n",
    "# print the 10 first words\n",
    "print(counts.take(10))\n",
    "\n",
    "sc.stop()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Pi Estimation\n",
    "Spark can also be used for compute-intensive tasks. This code estimates π by \"throwing darts\" at a circle. We pick random points in the unit square ((0, 0) to (1,1)) and see how many fall in the unit circle. The fraction should be $\\frac{π}{4}$, so we use this to get our estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.140732\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import random\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "num_samples = 1000000\n",
    "\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4*count/num_samples\n",
    "print(\"Pi is roughly %f\" % pi)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### Building a spam classifier locally, with PySpark and some Spark MLLib classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['You', 'have', '1', 'new', 'message.', 'Please', 'call', '08712400200.']]\n",
      "[['Rofl.', 'Its', 'true', 'to', 'its', 'name']]\n",
      "[SparseVector(200, {22: 2.0, 59: 1.0, 82: 1.0, 95: 1.0, 100: 1.0, 106: 1.0, 125: 1.0})]\n",
      "[SparseVector(200, {8: 1.0, 16: 1.0, 28: 1.0, 63: 1.0, 70: 1.0, 165: 1.0})]\n",
      "[LabeledPoint(1.0, (200,[22,59,82,95,100,106,125],[2.0,1.0,1.0,1.0,1.0,1.0,1.0]))]\n",
      "[LabeledPoint(0.0, (200,[8,16,28,63,70,165],[1.0,1.0,1.0,1.0,1.0,1.0]))]\n",
      "LogisticRegressionWithSGD accuracy = 0.86\n",
      "SVMWithSGD accuracy = 0.87\n",
      "NaiveBayes accuracy = 0.93\n",
      "DecisionTree accuracy = 0.89\n",
      "RandomForest accuracy = 0.87\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"Spam classifier with Spark MLLib classification algorithms\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD, SVMWithSGD, NaiveBayes\n",
    "from pyspark.mllib.tree import DecisionTree, RandomForest\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# Load 2 types of emails from text files: spam and ham (non-spam).\n",
    "# Each line has text from one email.\n",
    "\n",
    "spam = sc.textFile(\"data/spam\")\n",
    "ham = sc.textFile(\"data/ham\") \n",
    "\n",
    "spam_words = spam.map(lambda email: email.split())\n",
    "ham_words = ham.map(lambda email: email.split())\n",
    "\n",
    "print(spam_words.take(1))\n",
    "print(ham_words.take(1))\n",
    "\n",
    "# Create a HashingTF instance to map email text to vectors of features.\n",
    "tf = HashingTF(numFeatures = 200)\n",
    "spam_features = tf.transform(spam_words)\n",
    "ham_features = tf.transform(ham_words)\n",
    "\n",
    "print(spam_features.take(1))\n",
    "print(ham_features.take(1))\n",
    "\n",
    "# Create binary LabeledPoint datasets for positive (spam) and zero (ham) examples.\n",
    "spam_samples = spam_features.map(lambda features:LabeledPoint(1, features))\n",
    "ham_samples = ham_features.map(lambda features:LabeledPoint(0, features))\n",
    "\n",
    "print(spam_samples.take(1))\n",
    "print(ham_samples.take(1))\n",
    "\n",
    "#Split the data set 80/20\n",
    "samples = spam_samples.union(ham_samples)\n",
    "[training_data, test_data] = samples.randomSplit([0.8, 0.2])\n",
    "training_data.cache()\n",
    "test_data.cache()\n",
    "\n",
    "def score(model, test_data):\n",
    "    predictions = model.predict(test_data.map(lambda x: x.features))\n",
    "    labels_and_preds = test_data.map(lambda x: x.label).zip(predictions)\n",
    "    accuracy = labels_and_preds.filter(lambda x: x[0] == x[1]).count()/float(test_data.count())\n",
    "    return accuracy\n",
    "\n",
    "#Applied some classification algorithms \n",
    "MLA1 = [LogisticRegressionWithSGD(), SVMWithSGD(), NaiveBayes()]\n",
    "MLA2 = [DecisionTree()]\n",
    "MLA3 = [RandomForest()]\n",
    "\n",
    "MLA = MLA1+MLA2+MLA3\n",
    "\n",
    "for algo in MLA:\n",
    "    #set name and parameters\n",
    "    MLA_name = algo.__class__.__name__\n",
    "    if algo in MLA1:\n",
    "        model = algo.train(training_data)\n",
    "        print(MLA_name+' accuracy = {:.2f}'.format(score(model, test_data)))\n",
    "    elif algo in MLA2:\n",
    "        model = algo.trainClassifier(training_data,numClasses=2,categoricalFeaturesInfo={})\n",
    "        print(MLA_name+' accuracy = {:.2f}'.format(score(model, test_data)))\n",
    "    else:\n",
    "        model = algo.trainClassifier(training_data,numClasses=2,categoricalFeaturesInfo={},numTrees=16)\n",
    "        print(MLA_name+' accuracy = {:.2f}'.format(score(model, test_data)))\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame-based API Examples (``spark.ml`` package)\n",
    "In Spark, a DataFrame is a distributed collection of data organized into named columns. Users can use DataFrame API to perform various relational operations on both external data sources and Spark’s built-in distributed collections without providing specific procedures for processing data. Also, programs based on DataFrame API will be automatically optimized by Spark’s built-in optimizer, Catalyst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Automatic classification of texts\n",
    "We will create a text classification model that can predict what is the subject of a conversation. We will use a public dataset containing conversations that took place in Usenet newsgroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.78\n"
     ]
    }
   ],
   "source": [
    "# Instantiation of a SparkContext\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Start a SparkContext and a SparkSession\n",
    "confspark = SparkConf().setMaster(\"local[*]\").setAppName(\"Automatic classification of texts with Spark\")\n",
    "sc = SparkContext(conf=confspark)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import CountVectorizer, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def load_dataframe(path):\n",
    "    rdd = sc.textFile(path)\\\n",
    "        .map(lambda line: line.split())\\\n",
    "        .map(lambda words: Row(label=words[0], words=words[1:]))\n",
    "    return spark.createDataFrame(rdd)\n",
    "\n",
    "train_data = load_dataframe(\"data/20ng-train-all-terms.txt\")\n",
    "test_data = load_dataframe(\"data/20ng-test-all-terms.txt\")\n",
    "\n",
    "# Learn the vocabulary of our training data\n",
    "vectorizer = CountVectorizer(inputCol = \"words\", outputCol = \"bag_of_words\", vocabSize=6000)\n",
    "# Convert string labels to floats\n",
    "label_indexer = StringIndexer(inputCol = \"label\", outputCol = \"label_index\")\n",
    "# Learn multiclass classifier on training data\n",
    "classifier = NaiveBayes(labelCol=\"label_index\", featuresCol=\"bag_of_words\", predictionCol=\"label_index_predicted\")\n",
    "\n",
    "pipeline = Pipeline(stages=[vectorizer, label_indexer, classifier])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "pipeline_model = pipeline.fit(train_data)\n",
    "\n",
    "# Predict labels on test data\n",
    "test_predicted = pipeline_model.transform(test_data)\n",
    "\n",
    "# Classifier evaluation\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label_index\", predictionCol=\"label_index_predicted\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(test_predicted)\n",
    "print(\"Accuracy = {:.2f}\".format(accuracy))\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### Building a spam classifier locally, with PySpark and some Spark ML classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------+------+\n",
      "|message                                                                                                                      |status|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------+------+\n",
      "|Rofl. Its true to its name                                                                                                   |ham   |\n",
      "|The guy did some bitching but I acted like i'd be interested in buying something else next week and he gave it to us for free|ham   |\n",
      "|Pity, * was in mood for that. So...any other suggestions?                                                                    |ham   |\n",
      "|Will ü b going to esplanade fr home?                                                                                         |ham   |\n",
      "|Huh y lei...                                                                                                                 |ham   |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+------+\n",
      "|             message|status|\n",
      "+--------------------+------+\n",
      "|You have 1 new me...|  spam|\n",
      "|Urgent! Please ca...|  spam|\n",
      "|Dear 0776xxxxxxx ...|  spam|\n",
      "|U 447801259231 ha...|  spam|\n",
      "|Congrats! 2 mobil...|  spam|\n",
      "+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+-----------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|message                                                                                                                      |\n",
      "+-----+-----------------------------------------------------------------------------------------------------------------------------+\n",
      "|1.0  |Rofl. Its true to its name                                                                                                   |\n",
      "|1.0  |The guy did some bitching but I acted like i'd be interested in buying something else next week and he gave it to us for free|\n",
      "|1.0  |Pity, * was in mood for that. So...any other suggestions?                                                                    |\n",
      "|1.0  |Will ü b going to esplanade fr home?                                                                                         |\n",
      "|1.0  |Huh y lei...                                                                                                                 |\n",
      "+-----+-----------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+--------------------+--------------------+\n",
      "|label|             message|               words|\n",
      "+-----+--------------------+--------------------+\n",
      "|  1.0|Rofl. Its true to...|[rofl., its, true...|\n",
      "|  1.0|The guy did some ...|[the, guy, did, s...|\n",
      "|  1.0|Pity, * was in mo...|[pity,, *, was, i...|\n",
      "|  1.0|Will ü b going to...|[will, ü, b, goin...|\n",
      "|  1.0|        Huh y lei...|    [huh, y, lei...]|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "|label|             message|               words|        bag_of_words|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "|  1.0|Rofl. Its true to...|[rofl., its, true...|(6000,[0,59,296,7...|\n",
      "|  1.0|The guy did some ...|[the, guy, did, s...|(6000,[0,1,4,6,8,...|\n",
      "|  1.0|Pity, * was in mo...|[pity,, *, was, i...|(6000,[8,10,53,23...|\n",
      "|  1.0|Will ü b going to...|[will, ü, b, goin...|(6000,[0,30,74,76...|\n",
      "|  1.0|        Huh y lei...|    [huh, y, lei...]|(6000,[284,732,11...|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  1.0|(6000,[0,59,296,7...|\n",
      "|  1.0|(6000,[0,1,4,6,8,...|\n",
      "|  1.0|(6000,[8,10,53,23...|\n",
      "|  1.0|(6000,[0,30,74,76...|\n",
      "|  1.0|(6000,[284,732,11...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0|  588|\n",
      "|  1.0| 3866|\n",
      "+-----+-----+\n",
      "\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0|  159|\n",
      "|  1.0|  961|\n",
      "+-----+-----+\n",
      "\n",
      "LogisticRegression test set accuracy = 0.98\n",
      "RandomForestClassifier test set accuracy = 0.86\n",
      "NaiveBayes test set accuracy = 0.97\n",
      "DecisionTreeClassifier test set accuracy = 0.92\n",
      "GBTClassifier test set accuracy = 0.96\n",
      "MultilayerPerceptronClassifier test set accuracy = 0.98\n"
     ]
    }
   ],
   "source": [
    "#Instantiation and start of a SparkSession\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Spam classifier with Spark ML classification algorithms\").getOrCreate()\n",
    "\n",
    "#Read Data\n",
    "df_ham = spark.read.csv(\"data/ham\", sep = \"\\t\", inferSchema=True, header = False)\n",
    "df_spam = spark.read.csv(\"data/spam\", sep = \"\\t\", inferSchema=True, header = False)\n",
    "\n",
    "#Add column to designate the message status\n",
    "from pyspark.sql.functions import lit, concat\n",
    "\n",
    "df_ham = df_ham.withColumn(\"status\",lit(\"ham\"))\n",
    "df_spam = df_spam.withColumn(\"status\",lit(\"spam\"))\n",
    "\n",
    "#Rename Columns\n",
    "df_ham = df_ham.withColumnRenamed('_c0', 'message').withColumnRenamed('status', 'status')\n",
    "df_ham.show(5, truncate = False)\n",
    "df_spam = df_spam.withColumnRenamed('_c0', 'message').withColumnRenamed('status', 'status')\n",
    "df_spam.show(5)\n",
    "\n",
    "#Create a unique dataframe\n",
    "df = df_ham.unionAll(df_spam)\n",
    "\n",
    "#Change the status column to numeric\n",
    "df.createOrReplaceTempView('temp')\n",
    "df = spark.sql('select case status when \"ham\" then 1.0  else 0 end as label, message from temp')\n",
    "df.show(5, truncate = False)\n",
    "\n",
    "#Tokenize the messages\n",
    "from pyspark.ml.feature import  Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"message\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(df)\n",
    "wordsData.show(5)\n",
    "\n",
    "#Apply CountVectorizer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "#count = CountVectorizer(inputCol=\"words\", outputCol=\"bag_of_words\")\n",
    "count = CountVectorizer(inputCol=\"words\", outputCol=\"bag_of_words\", vocabSize=6000)\n",
    "model = count.fit(wordsData)\n",
    "df_bag_of_words = model.transform(wordsData)\n",
    "df_bag_of_words.show(5)\n",
    "\n",
    "#Apply term frequency–inverse document frequency (TF-IDF)\n",
    "from pyspark.ml.feature import  IDF\n",
    "idf = IDF(inputCol=\"bag_of_words\", outputCol=\"features\")\n",
    "idfModel = idf.fit(df_bag_of_words)\n",
    "idfData = idfModel.transform(df_bag_of_words)\n",
    "idfData.select(\"label\", \"features\").show(5)  \n",
    "\n",
    "#Split data into training (80%) and testing (20%)\n",
    "seed = 0  # set seed for reproducibility\n",
    "trainDF, testDF = idfData.randomSplit([0.8,0.2], seed)\n",
    "\n",
    "trainDF.groupBy('label').count().show()\n",
    "testDF.groupBy('label').count().show()\n",
    "\n",
    "\n",
    "#Applied some classification algorithms \n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "# Creation of the classification models.\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "rf = RandomForestClassifier(numTrees=10)\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "dt = DecisionTreeClassifier(maxDepth=2)\n",
    "gbt = GBTClassifier(maxIter=10)\n",
    "mpc = MultilayerPerceptronClassifier(maxIter=100, layers=[count.getVocabSize(), 15, 6, 2], blockSize=128)\n",
    "\n",
    "MLA = [lr, rf, nb, dt, gbt, mpc]\n",
    "\n",
    "for algo in MLA:\n",
    "    #set name and parameters\n",
    "    MLA_name = algo.__class__.__name__\n",
    "    # train the model\n",
    "    model = algo.fit(trainDF)\n",
    "    #Predict using the test data and evaluate the predictions\n",
    "    predictions = model.transform(testDF)\n",
    "    # compute accuracy on the test set\n",
    "    my_mc = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='accuracy')\n",
    "    accuracy = my_mc.evaluate(predictions)\n",
    "    #print(\"Test set accuracy = \" + str(accuracy))\n",
    "    print(MLA_name+' test set accuracy = {:.2f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Text Search\n",
    "In this example, we search through the authentication failures in a log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=Spam classifier with Spark ML classification algorithms, master=local[*]) created by getOrCreate at <ipython-input-6-7dfaa6b688a6>:7 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-2a9d2a9bd76c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mappName\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"1st example: Text Search\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparkSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Hadoop\\spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \"\"\"\n\u001b[0;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32mC:\\Hadoop\\spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    306\u001b[0m                         \u001b[1;34m\" created by %s at %s:%s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[1;32m--> 308\u001b[1;33m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[0;32m    309\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Spam classifier with Spark ML classification algorithms, master=local[*]) created by getOrCreate at <ipython-input-6-7dfaa6b688a6>:7 "
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"1st example: Text Search\")\n",
    "spark = pyspark.sql.SparkSession(sc)\n",
    "\n",
    "textFile = sc.textFile(\"data/Linux_2k.log\")\n",
    "\n",
    "# Creates a DataFrame having a single column named \"line\"\n",
    "\n",
    "df = textFile.map(lambda s: s.split(\"\\n\")).toDF([\"line\"])\n",
    "\n",
    "df.show(4)\n",
    "\n",
    "# Counts lines mentioning \"authentication failure\"\n",
    "failures = df.filter(df.line.like(\"%authentication failure%\"))\n",
    "\n",
    "print('Number of lines mentioning \"authentication failure\":', failures.count())\n",
    "\n",
    "# Counts authentication failures mentioning \"ruser= rhost=218.188.2.4\"\n",
    "\n",
    "print('Number of authentication failures mentioning \"ruser= rhost=218.188.2.4\":', failures.filter(failures.line.like(\"%ruser= rhost=218.188.2.4%\")).count())\n",
    "\n",
    "# Fetches the ruser= rhost=218.188.2.4 authentication failures as an array of strings\n",
    "failures.filter(df.line.like(\"%ruser= rhost=218.188.2.4%\")).collect()\n",
    "\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
